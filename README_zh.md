# EETQ

EETQ(Easy & Efficient Quantization for Transformers)æ˜¯ä¸€æ¬¾é’ˆå¯¹transformeræ¨¡å‹çš„é‡åŒ–å·¥å…·

## ç›®å½•

- [EETQ](#eetq)
  - [ç›®å½•](#ç›®å½•)
  - [ç‰¹ç‚¹](#ç‰¹ç‚¹)
  - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
    - [ç¯å¢ƒ](#ç¯å¢ƒ)
    - [å®‰è£…](#å®‰è£…)
    - [ä½¿ç”¨](#ä½¿ç”¨)
  - [å‚è€ƒç”¨ä¾‹](#å‚è€ƒç”¨ä¾‹)
  - [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)

## ç‰¹ç‚¹
- æ–°ç‰¹æ€§ğŸ”¥: [å¼•å…¥gemvç®—å­](https://github.com/huggingface/text-generation-inference/pull/1502) æå‡æ€§èƒ½10%~30%. 

- é«˜æ€§èƒ½çš„INT8æƒé‡è®­ç»ƒåé‡åŒ–ç®—å­

  * æå–è‡ª[FasterTransformer](https://github.com/NVIDIA/FasterTransformer/tree/main/src/fastertransformer/kernels/cutlass_kernels/fpA_intB_gemm)çš„é«˜æ€§èƒ½GEMMå†…æ ¸ï¼Œå¯ä»¥æ›´åŠ æ–¹ä¾¿é›†æˆè‡³æ‚¨çš„é¡¹ç›®ä¸­

  * æ— éœ€é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ

- ä½¿ç”¨[Flash-Attention V2](https://github.com/Dao-AILab/flash-attention)ä¼˜åŒ–attentionçš„æ¨ç†æ€§èƒ½

- ç®€å•æ˜“ç”¨ï¼Œåªéœ€ä¸€è¡Œä»£ç å³å¯é€‚é…æ‚¨çš„PyTorchæ¨¡å‹
## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒ

* cuda:>=11.4
* python:>=3.8 
* gcc:>= 7.4.0 
* torch:>=1.14.0 
* transformers:>=4.27.0

### å®‰è£…
æ¨èä½¿ç”¨Dockerfile.
```bash
$ git clone https://github.com/NetEase-FuXi/EETQ.git
$ cd EETQ/
$ git submodule update --init --recursive
$ pip install .
```
å¦‚æœæ‚¨çš„è®¾å¤‡å†…å­˜å°äº96GBï¼Œå¹¶ä¸”CPUæ ¸æ•°å¾ˆå¤šï¼ŒNinjaå¯èƒ½ä¼šè¿è¡Œè¿‡å¤šçš„å¹¶è¡Œç¼–è¯‘ä»»åŠ¡ï¼Œå¯èƒ½ä¼šè€—å°½è®¾å¤‡å†…å­˜ã€‚ä¸ºäº†é™åˆ¶å¹¶è¡Œç¼–è¯‘ä»»åŠ¡çš„æ•°é‡ï¼Œæ‚¨å¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡MAX_JOBSï¼š
```bash
$ MAX_JOBS=4 pip install .
```
### ä½¿ç”¨
1. é‡åŒ–torchæ¨¡å‹
```python
from eetq.utils import eet_quantize
eet_quantize(torch_model, init_only=False, include=[nn.Linear], exclude=["lm_head"], device="cuda:0")
```


2. é‡åŒ–torchæ¨¡å‹å¹¶ä½¿ç”¨flash attentionä¼˜åŒ–
```python
...
model = AutoModelForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16)
from eetq.utils import eet_accelerator
eet_accelerator(model, quantize=True, fused_attn=True, dev="cuda:0")
model.to("cuda:0")

# æ¨ç†
res = model.generate(...)

```

3. åœ¨[TGI](https://github.com/huggingface/text-generation-inference)ä¸­ä½¿ç”¨eetqè¿›è¡Œé‡åŒ–åŠ é€Ÿï¼Œ[PRé“¾æ¥](https://github.com/huggingface/text-generation-inference/pull/1068)
```bash
text-generation-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize eetq ...
```

4. åœ¨[LoRAX](https://github.com/predibase/lorax)ä¸­ä½¿ç”¨EETQ. å‚è€ƒ[æ–‡æ¡£](https://predibase.github.io/lorax/guides/quantization/#eetq).
```bash
lorax-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize eetq ...
```

## å‚è€ƒç”¨ä¾‹

- [examples/models/llama_transformers_example.py](examples/models/llama_transformers_example.py)

## æ€§èƒ½æµ‹è¯•

- llama-13b (test on 3090)
prompt=1024, max_new_tokens=50
<img src="./docs/images/benchmark.jpg" style="zoom:50%;" />